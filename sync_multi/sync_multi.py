#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import fnmatch
import hashlib
import json
import logging
import os
import platform
import random
import shutil
import signal
import sys
import tempfile
import threading
import time
from logging.handlers import RotatingFileHandler
from multiprocessing import Process
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer
from watchdog.observers.polling import PollingObserver

# â€”â€” çŒ«å¨˜å°¾å·´ â€”â€”
CAT_TAILS = ["å–µï½", "å–µâ™¡ï½", "å‘œå–µï½", "å™œï½"]
def random_tail() -> str:
    return random.choice(CAT_TAILS)

# â€”â€” 8. èµ„æºé™åˆ¶ â€”â€”
try:
    import resource
    resource.setrlimit(resource.RLIMIT_AS, (1 << 30, 1 << 30))
    resource.setrlimit(resource.RLIMIT_CPU, (3600, 3600))
except Exception:
    pass

CFG_PATH = Path("config.json")
DEBOUNCE = 1.0
HEARTBEAT_INTERVAL = 3600
RESTART_DELAY = 5

# â€”â€” çŒ«å¨˜æ—¥å¿—æ ¼å¼ + è½®è½¬ â€”â€”
class CatFormatter(logging.Formatter):
    def format(self, record):
        ct = self.formatTime(record, "%Y-%m-%d %H:%M:%S")
        return f"{ct} | {record.levelname:^5} | {record.getMessage()} {random_tail()}"

def setup_logger(name: str, logfile: Path) -> logging.Logger:
    logfile.parent.mkdir(parents=True, exist_ok=True)
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    logger.propagate = False
    if not logger.handlers:
        fmt = CatFormatter()
        fh = RotatingFileHandler(
            logfile, maxBytes=10*1024*1024, backupCount=5, encoding="utf-8"
        )
        fh.setFormatter(fmt)
        sh = logging.StreamHandler()
        sh.setFormatter(fmt)
        logger.addHandler(fh)
        logger.addHandler(sh)
    return logger

def retry(times=3, delay=0.5):
    def deco(fn):
        def wrapper(*a, **kw):
            for i in range(times):
                try:
                    return fn(*a, **kw)
                except Exception:
                    if i < times - 1:
                        time.sleep(delay)
                    else:
                        raise
        return wrapper
    return deco

def compute_hash(path: Path, algo="sha256", chunk_size=8192) -> str:
    h = hashlib.new(algo)
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()

class SyncTask:
    def __init__(self, cfg: dict):
        self.name    = cfg.get("name", "sync_task")
        srcs = cfg.get("sources") or [cfg.get("source")]
        tgts = cfg.get("targets") or [cfg.get("target")]
        self.sources = [Path(p) for p in srcs if p]
        self.targets = [Path(p) for p in tgts if p]
        self.exclude = cfg.get("exclude", [])
        self.workers = cfg.get("workers", 4)
        self.logfile = Path(cfg.get("log", f"logs/{self.name}.log"))

        # åŒæ­¥æ§åˆ¶
        self._lock            = threading.Lock()
        self._timer           = None
        self._pending         = False
        self._paths_lock      = threading.Lock()
        self._pending_paths   = set()
        self._counter_lock    = threading.Lock()
        self._copy_count      = 0
        self._delete_count    = 0

        self.logger = setup_logger(self.name, self.logfile)
        self._validate()
        self.logger.info(f"ğŸŸ¢ å¯åŠ¨ä»»åŠ¡ã€Œ{self.name}ã€")

    def _validate(self):
        if not (self.sources and self.targets):
            raise ValueError("éœ€è‡³å°‘ä¸€ä¸ªæºå’Œä¸€ä¸ªç›®æ ‡")
        for s in self.sources:
            if not s.is_dir():
                raise ValueError(f"æºä¸å­˜åœ¨ï¼š{s}")
        for t in self.targets:
            t.mkdir(parents=True, exist_ok=True)
            test = t / f".sync_test_{int(time.time())}"
            try:
                test.write_text("ok"); test.unlink()
            except Exception as e:
                raise ValueError(f"ç›®æ ‡ä¸å¯å†™ï¼š{t}ï¼›{e}")

    def _pairs(self):
        if len(self.sources) == len(self.targets):
            return list(zip(self.sources, self.targets))
        if len(self.sources) == 1:
            return [(self.sources[0], t) for t in self.targets]
        return [(s, self.targets[0]) for s in self.sources]

    def should_exclude(self, path: Path, base: Path) -> bool:
        rel = path.relative_to(base).as_posix()
        return any(fnmatch.fnmatch(rel, pat) for pat in self.exclude)

    def cleanup_tmp_files(self):
        for _, t_base in self._pairs():
            for tmp in t_base.rglob("*.sync_tmp*"):
                try:
                    tmp.unlink()
                    self.logger.info(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼š{tmp}")
                except: pass

    @retry(times=3, delay=0.3)
    def _atomic_copy(self, src: Path, dst: Path):
        if src.is_symlink():
            target = os.readlink(src)
            try: dst.unlink()
            except: pass
            os.symlink(target, dst)
            try: shutil.copystat(src, dst, follow_symlinks=False)
            except: pass
            return
        dst.parent.mkdir(parents=True, exist_ok=True)
        tmp = tempfile.NamedTemporaryFile(dir=dst.parent, delete=False)
        try:
            with src.open("rb") as fsrc, tmp:
                shutil.copyfileobj(fsrc, tmp); tmp.flush()
            Path(tmp.name).replace(dst)
            try: shutil.copystat(src, dst, follow_symlinks=False)
            except: pass
        finally:
            if tmp and Path(tmp.name).exists():
                try: Path(tmp.name).unlink()
                except: pass

    @retry(times=3, delay=0.3)
    def _safe_delete(self, path: Path):
        if path.is_dir():
            path.rmdir()
        else:
            path.unlink()

    def _wrapped_copy(self, src, dst, sem):
        try:
            self._atomic_copy(src, dst)
            with self._counter_lock:
                self._copy_count += 1
            self.logger.info(f"ğŸ“„ å¤åˆ¶: {src} â†’ {dst}")
        finally:
            sem.release()

    def _wrapped_delete(self, path, sem):
        try:
            self._safe_delete(path)
            with self._counter_lock:
                self._delete_count += 1
            self.logger.info(f"ğŸ—‘ åˆ é™¤: {path}")
        finally:
            sem.release()

    def sync(self):
        # æ‰¹é‡å˜åŠ¨æ±‡æ€»
        with self._paths_lock:
            changed = list(self._pending_paths)
            self._pending_paths.clear()
        if changed:
            txt = "; ".join(str(p) for p in changed)
            self.logger.info(f"âœ¨ æ£€æµ‹åˆ°å˜åŠ¨ {len(changed)} æ¡: {txt}")

        if not self._lock.acquire(False):
            self._pending = True
            return

        with self._counter_lock:
            self._copy_count   = 0
            self._delete_count = 0

        start = time.time()
        self.logger.info("ğŸ•’ å¼€å§‹åŒæ­¥")
        sem = threading.Semaphore(self.workers * 2)
        try:
            with ThreadPoolExecutor(max_workers=self.workers) as pool:
                for s_base, t_base in self._pairs():
                    for src in s_base.rglob("*"):
                        try:
                            if (src.is_file()
                                and not self.should_exclude(src, s_base)):
                                dst = t_base / src.relative_to(s_base)
                                need = False
                                if not dst.exists():
                                    need = True
                                else:
                                    if src.stat().st_mtime > dst.stat().st_mtime:
                                        if compute_hash(src) != compute_hash(dst):
                                            need = True
                                if need:
                                    sem.acquire()
                                    pool.submit(
                                        self._wrapped_copy, src, dst, sem
                                    )
                        except: continue
                    for dst in t_base.rglob("*"):
                        try:
                            rel = dst.relative_to(t_base).as_posix()
                            if any(fnmatch.fnmatch(rel, pat)
                                   for pat in self.exclude):
                                continue
                            src = s_base / rel
                            if not src.exists():
                                sem.acquire()
                                pool.submit(
                                    self._wrapped_delete, dst, sem
                                )
                        except: continue
                pool.shutdown(wait=True)

            elapsed = time.time() - start
            self.logger.info(
                f"âœ… åŒæ­¥å®Œæˆï¼šå¤åˆ¶ {self._copy_count} ä¸ªï¼Œ"
                f"åˆ é™¤ {self._delete_count} ä¸ªï¼Œè€—æ—¶ {elapsed:.2f}s"
            )
        except Exception as e:
            self.logger.error(f"åŒæ­¥å‡ºé”™ï¼š{e}", exc_info=True)
        finally:
            self._lock.release()
            if self._pending:
                self._pending = False
                self.sync()

    class Handler(FileSystemEventHandler):
        def __init__(self, task):
            self.task = task

        def on_any_event(self, event):
            with self.task._paths_lock:
                self.task._pending_paths.add(Path(event.src_path))
            self.task.sync()
            if self.task._timer and self.task._timer.is_alive():
                self.task._timer.cancel()
            self.task._timer = threading.Timer(
                DEBOUNCE, self.task.sync)
            self.task._timer.start()

    def _heartbeat_loop(self):
        while True:
            time.sleep(HEARTBEAT_INTERVAL)
            self.logger.info(f"ğŸ”„ å¿ƒè·³ï¼šä»»åŠ¡ã€Œ{self.name}ã€æ­£å¸¸è¿è¡Œ")

    def start(self):
        self.cleanup_tmp_files()
        self.sync()
        threading.Thread(
            target=self._heartbeat_loop, daemon=True
        ).start()
        obs_list = []
        for s in self.sources:
            ObsCls = PollingObserver if (
                platform.system() == "Darwin"
            ) else Observer
            obs = ObsCls()
            obs.schedule(self.Handler(self), str(s), recursive=True)
            obs.start()
            self.logger.info(f"ğŸ‘€ ç›‘å¬: {s}")
            obs_list.append(obs)
        return obs_list

# â€”â€” åŠ¨æ€é‡è½½é…ç½® â€”â€”
tasks: list[SyncTask] = []
observers: list = []

class ConfigReloader(FileSystemEventHandler):
    def __init__(self):
        super().__init__()
        self._timer = None

    def on_modified(self, event):
        if Path(event.src_path).resolve() == CFG_PATH.resolve():
            if self._timer and self._timer.is_alive():
                self._timer.cancel()
            self._timer = threading.Timer(DEBOUNCE, reload_config)
            self._timer.start()

def reload_config():
    logging.info("ğŸ”„ é…ç½®å˜æ›´ï¼Œé‡æ–°åŠ è½½ä»»åŠ¡")
    for o in observers:
        o.stop()
    for o in observers:
        o.join()
    observers.clear()
    tasks.clear()
    try:
        cfg = json.loads(CFG_PATH.read_text(encoding="utf-8"))
        for tcfg in cfg.get("tasks", []):
            try:
                task = SyncTask(tcfg)
                tasks.append(task)
                for o in task.start():
                    observers.append(o)
            except Exception as e:
                logging.error(f"ä»»åŠ¡åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
    except Exception as e:
        logging.error(f"åŠ è½½ config.json å¤±è´¥ï¼š{e}")

def sync_worker():
    logger = logging.getLogger("sync_worker")
    try:
        cfg_obs = PollingObserver()
        cfg_obs.schedule(ConfigReloader(), str(CFG_PATH.parent),
                         recursive=False)
        cfg_obs.start()
        observers.append(cfg_obs)
        reload_config()
        while True:
            time.sleep(0.5)
    except KeyboardInterrupt:
        logger.info("å­è¿›ç¨‹æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œä¼˜é›…é€€å‡º")
    finally:
        for o in observers:
            o.stop()
        for o in observers:
            o.join()

def supervise():
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    handler.setFormatter(CatFormatter())
    root_logger.addHandler(handler)
    try:
        while True:
            p = Process(target=sync_worker, name="sync_worker")
            p.start()
            p.join()
            root_logger.error(
                f"ğŸš¨ å­è¿›ç¨‹é€€å‡º(code={p.exitcode})ï¼Œ{RESTART_DELAY}s åé‡å¯"
            )
            time.sleep(RESTART_DELAY)
    except KeyboardInterrupt:
        root_logger.info("çˆ¶è¿›ç¨‹æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œä¼˜é›…é€€å‡º")
        sys.exit(0)

if __name__ == "__main__":
    supervise()
